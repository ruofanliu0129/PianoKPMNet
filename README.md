# From Pose to Muscle: Multimodal Learning for Piano Hand Muscle Electromyography
<a href=""><img src="https://img.shields.io/badge/Paper-arXiv-red"></a>
<a href="https://drive.google.com/drive/folders/142jhYuR6wSfrPjj5P0GRS-1Tay8zJWCx?usp=sharing"><img src="https://img.shields.io/badge/Dataset-Google%20Drive-blue?logo=googledrive"></a>
<a href="#bibtex"><img src="https://img.shields.io/badge/BibTeX-Citation-yellow"></a>

A multimodal dataset, [PianoKPM Dataset](https://drive.google.com/drive/folders/142jhYuR6wSfrPjj5P0GRS-1Tay8zJWCx?usp=sharing), and a hand muscle estimation framework, [PianoKPM Net](https://github.com/ruofanliu0129/PianoKPMNet.git), are introduced to facilitate high-fidelity EMG inference. 

[PianoKPM Dataset](https://drive.google.com/drive/folders/142jhYuR6wSfrPjj5P0GRS-1Tay8zJWCx?usp=sharing):
<img src="images/PianoKPM_Dataset.png" alt="dataset" style="width:100%;"/>
[PianoKPM Net](https://github.com/ruofanliu0129/PianoKPMNet.git):
<img src="images/PianoKPM_Net.png" alt="net" style="width:100%;"/>

Installation setup, dataset downloading, pre-trained models, training, and inference code are provided.

## Installation and Setup

Clone the PianoKPMNet:
```setup
git clone https://github.com/ruofanliu0129/PianoKPMNet.git
cd PianoKPMNet/
```
Create and activate the environment:
```setup
conda env create -f environment.yml
conda activate PianoKPM
```
*Note: We recommended running on a Linux x86_64 (amd64) architecture. The following instructions are tested on an Ubuntu 22.04 Distro.*



## PianoKPM Dataset

Download PianoKPM V1.0 full dataset [HERE](https://drive.google.com/drive/folders/142jhYuR6wSfrPjj5P0GRS-1Tay8zJWCx?usp=sharing).
```dataset
cd /path/to/dataset
unzip hand_data.zip
unzip keystroke_data.zip
unzip emg_data.zip
```

Folder structure:
```dataset
/path/to/dataset
â”œâ”€â”€ hand_data/
â”‚   â”œâ”€â”€ keyp_top/
â”‚   â”‚   â”œâ”€â”€ t1_p1_1.pkl
â”‚   â”‚   â”œâ”€â”€ t1_p1_2.pkl
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ keyp_right/
â”‚   â”‚   â”œâ”€â”€ t1_p1_1.pkl
â”‚   â”‚   â”œâ”€â”€ t1_p1_2.pkl
â”‚   â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ keystroke_data/
â”‚   â”œâ”€â”€ t1_p1_1.pkl
â”‚   â”œâ”€â”€ t1_p1_2.pkl
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ emg_data/
â”‚   â”œâ”€â”€ t1_p1_1.pkl
â”‚   â”œâ”€â”€ t1_p1_2.pkl
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ dataset_architectural.json
â”œâ”€â”€ dataset_held_out.json
```
*Two dataset split configurations are provided:*
- *`dataset_architectural.json`: used in the architectural evaluation (Section 5.1).*
- *`dataset_held_out.json`: used in the held-out evaluation (Section 5.2).*



## Pre-trained Models

Download pretrained models [HERE](https://drive.google.com/file/d/1Pk2XhcR7j-L9ozin2YiN5SPJrmkpOyOd/view?usp=sharing).
```pretrained
/path/to/saved_model
â”œâ”€â”€ architectural_eval_model.pth
â”œâ”€â”€ heldout_eval_model.pth
```
*Two pretrained models are provided:*
- *`architectural_eval_model.pth`: used in the architectural evaluation (Section 5.1).*
- *`heldout_eval_model.pth`: used in the held-out evaluation (Section 5.2).*



## Configuration Update

Edit `PianoKPMNet/config/base.yaml`:
1. Set `data_root` to your dataset path.
2. Update `dataset_split` to your dataset split configuration file path.
5. (Optional) Adjust training hyperparameters like `train.epoch`, `train.batch_size`, `train.lr` etc.
4. (Optional) Update `checkpoint_path` and `checkpoint_name` to specify the pre-trained model.
4. (Optional) Choose `inference.test_set` to evaluate different test sets (e.g., "test", "test_cross_user", "test_cross_task").

## Training

To train the PianoKPM Net in the paper, run this command:
```train
python train.py
```
This will create a `ckpt` folder with the following structure:
```
/path/to/ckpt/yyyy-mm-dd/HH-MM-SS
â”œâ”€â”€ pianokpm.log
â”œâ”€â”€ model/ 
â”‚   â”œâ”€â”€ saved best models
â”œâ”€â”€ train_plots/ 
â”‚   â”œâ”€â”€ E0020/
â”‚   â”‚   â”œâ”€â”€ plots in training set at epoch 20
â”‚   â”œâ”€â”€ E0040/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ eval_plots/ 
â”‚   â”œâ”€â”€ E0020/
â”‚   â”‚   â”œâ”€â”€ plots in validation set at epoch 40
â”‚   â”œâ”€â”€ E0040/
â”‚   â”œâ”€â”€ ...
```


## Evaluation

To evaluate a model on PianoKPM Net, run:

```eval
python inference.py
```
This will create a `ckpt` folder with the following structure:
```
/path/to/ckpt/yyyy-mm-dd/HH-MM-SS
â”œâ”€â”€ pianokpm.log
â”œâ”€â”€ inference_emg/ 
â”‚   â”œâ”€â”€  estimated EMG sequences
â”œâ”€â”€ inference_plots/ 
â”‚   â”œâ”€â”€ visualization results for keystroke motions, GT EMG, and estimated EMG
```


## Results

Our model achieves the following performance on PianoKPM Dataset:

| Evaluation | Model name         | Test set | RMSE  | OTD |
| ------------------| ------------------ |---------------- |---------------- | -------------- |
| Architectural evaluation | architectural_eval_model.pth   |  test  |   0.134         |      0.031       |
| Held-out evaluation | heldout_eval_model.pth   |    test_cross_user | 0.209         |      0.095       |
| Held-out evaluation | heldout_eval_model.pth   |    test_cross_task | 0.264         |      0.152       |

<img src="images/Visualization_Results.png" alt="visualization" style="width:50%;"/>


## ðŸ“– BibTeX

```bibtex
@inproceedings{
anonymous2025from,
title={From Pose to Muscle: Multimodal Learning for Piano Hand Muscle Electromyography},
author={Anonymous},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=ftZEltGArK}
}
